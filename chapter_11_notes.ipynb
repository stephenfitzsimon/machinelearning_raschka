{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e9e985-964f-4670-83fc-f9ee782ae370",
   "metadata": {},
   "source": [
    "# Chapter 11 : Implementing a Multilayer Artificial Neural Network from Scratch\n",
    "\n",
    "## Modeling complex functions with artificial neural networks\n",
    "\n",
    "Note that a *neural network* originates in the 1940s with models such as ADALINE and the perceptron, already covered in this book.  Neural networks have become more populated after the ability to create *deep neural networks* which have multiple layers of neurons.\n",
    "\n",
    "### Single layer neural network recap\n",
    "\n",
    "For example, consider ADALINE.  Recall that in every epoch, the weight vector, $w$, and bias unit, $b$, are updated. Where\n",
    "$$\n",
    "    w := w + \\Delta w \\text{ and } b := b + \\Delta b\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta w_j = -\\eta \\frac{\\partial L}{\\partial w_j}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta b = -\\eta\\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "This is done through multiple passes over the training set where an activiation function (in ADALINE, the identity function) output is compared with the actual value.  Recall that this takes the opposite direction of the loss gradient ($\\nabla L(w)$) to find optimal weights of the model.  Generally $L$ is defined as the mean of square errors.  The model learning phase is accelerated by stochastic gradient descent.\n",
    "\n",
    "This will be used to implement and train a *multilayer perceptron* (MLP) model.\n",
    "\n",
    "### Introducing the multilayer neural network architecture\n",
    "\n",
    "A MLP model consists in an input layer, one or more hidden layers, and an output payer. If a network has more than one hidden layer, then it is said to be a *deep NN*. \n",
    "\n",
    "The $i$th activation unit in the $l$th later is denoted $a_i^{(l)}$.  Numerical units are usally not used for the layers; therfore, $x_i^{(in)}$ refers to the $i$th input feature value, $a_i^{(h)}$ refers to the $i$th unit in the hidden layer, and $a_i^{(out)}$ refers to the $i$th unit in the output layer. $b^{(h)}$, $b^{(out)}$ denote bias unit vectors storing $d$, the number of nodes, bias units for the hidden and output layers.\n",
    "\n",
    "### Activating a neural network via forward propagation\n",
    "\n",
    "The MLP procedure is summarized in three steps:\n",
    "1. Starting at $x_i^{(in)}$, patterns are forward propagated through the network\n",
    "2. Based on network output, calculate the loss we want to minimize using a loss function\n",
    "3. Back propogate the loss, finding the derivative of each weight and bias unity, updating the model\n",
    "These steps are repeated for each epoch. \n",
    "\n",
    "Moving step by step as follows.  First, calcualte the activation unit for the hidden layer as follows:\n",
    "$$\n",
    "    z_1^{(h)} = x_1^{(in)}w_{1,1}^{(h)} + \\dots + x_m^{(in)}w_{1,m}^{(h)}\n",
    "$$\n",
    "thus,\n",
    "$$\n",
    "    a_1^{(h)} = \\sigma(z_1^{(h)})\n",
    "$$\n",
    "Note that complex problems need a nonlinear activation function.  For example, the logistic regression's sigmoid activation function.\n",
    "\n",
    "The calculation can be generalized to $n$ examples as follows:\n",
    "$$\n",
    "    Z^{(h)}= X^{(in)}W^{{h)T} + b^{(h)}\n",
    "$$\n",
    "Thus:\n",
    "$$\n",
    "    A^{(h)} = \\sigma(Z^{(h)})\n",
    "$$\n",
    "And finally:\n",
    "$$\n",
    "    A^{(out)} = \\sigma(Z^{(out)})\n",
    "$$\n",
    "\n",
    "## Classifying handwritten digits\n",
    "\n",
    "Before implementing a multilayer perceptron, prepare the handwritten digits data set.\n",
    "\n",
    "Get data set from sci-kit learn.print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6f9e0c-d5b0-4d13-b7ce-0e1b1918460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml \n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048490a3-fd70-47e2-bdca-f1a1dbaf30df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c6f5a2-30c0-4005-a3c4-fa70f40491a7",
   "metadata": {},
   "source": [
    "Normalize the pixels. Recall that gradient-based optimization is better using normalized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a197b9-6d62-4e7b-abee-ca46e8498e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a404f0-7565-4026-a445-c46f0a09ceb6",
   "metadata": {},
   "source": [
    "Split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5817e32-5b11-4935-963d-64d571e9eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=10000, random_state=123, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedcc0bd-b2c9-4424-b9df-f685d3551b9b",
   "metadata": {},
   "source": [
    "### Implementing a multilayer perceptron\n",
    "\n",
    "This will implement a MLP with a single hidden layer.\n",
    "\n",
    "Write helper functions to calculate sigmoid function and to encode labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43184f2-69ac-444b-8915-43fb937b737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "    return ary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be736b-575a-4443-96fa-0fd3a7a570da",
   "metadata": {},
   "source": [
    "Make the NeuralNetMLP class.\n",
    "\n",
    "Notes:\n",
    "1. `__init__` : This instantiates the weights for both the output and hidden layers\n",
    "2. `.forward` : Uses training examples to make a prediction. `a_h` contains the activation values of the hidden layer, and `a_out` contains the class membership probabilities\n",
    "3. `.backward`  : Updates the weight and bias parameters of the network.  This is called the *backpropagation* algorithm.  This is done by calculating the loss gradient, and returning the updated bias and weights for the hidden layer and the membership probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7234a7b6-af8c-4822-a08a-eebc1bf59602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP:\n",
    "    def __init__(self, num_features, num_hidden,\n",
    "                 num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # hidden\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        # Hidden layer\n",
    "        # input dim: [n_hidden, n_features]\n",
    "        # dot [n_features, n_examples] .T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h \n",
    "        a_h = sigmoid(z_h)\n",
    "        # Output layer\n",
    "        # input dim: [n_classes, n_hidden]\n",
    "        # dot [n_hidden, n_examples] .T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out \n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_h, a_out\n",
    "    \n",
    "    def backward(self, x, a_h, a_out, y):\n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "        # one-hot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "        \n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0] \n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) \n",
    "        # sigmoid derivative # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out\n",
    "        # gradient for output weights\n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h\n",
    "        # input dim: [n_classes, n_examples]\n",
    "        # dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out) \n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "        \n",
    "        #################################\n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet\n",
    "        #    * dHiddenNet/dWeight\n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) \n",
    "        # sigmoid derivative\n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T,\n",
    "                        d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "        return (d_loss__dw_out, d_loss__db_out, d_loss__d_w_h, d_loss__d_b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d1f642-4b72-4469-a408-71c52238d2c0",
   "metadata": {},
   "source": [
    "Initialize a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c19a79-12bf-47bf-af18-274e5cebf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetMLP(num_features=28*28,\n",
    "    num_hidden=50,\n",
    "    num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35163d5-f751-4acd-bd09-d3c200e2a3ca",
   "metadata": {},
   "source": [
    "### Coding the Neural Network training loop\n",
    "\n",
    "Define a helper function to generate mini matches of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a5fea88-6b96-41f8-930f-a9b19b0f5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size] \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "        \n",
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(\n",
    "    targets, num_labels=num_labels\n",
    "    )\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db59ce2-1622-445c-9f4e-18e48abd5340",
   "metadata": {},
   "source": [
    "Get the minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "141be98a-7b97-42ab-af02-b2a92a969ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    minibatch_gen = minibatch_generator(X_train, y_train, minibatch_size)\n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "        break\n",
    "    break\n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba3370-ee13-47ba-9c7e-faf75834e536",
   "metadata": {},
   "source": [
    "Now predict on the model using `.forward`.  This returns the hidden and output layer activations.  From this, the accuracy can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "967e8fdd-9c6b-4b9f-b44e-1c9bb005a824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation MSE: 0.3\n",
      "Initial validation accuracy: 9.4%\n"
     ]
    }
   ],
   "source": [
    "_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da09e8a6-2942-411e-964d-40109f4ca09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10,\n",
    "    minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "        _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        onehot_targets = int_to_onehot(\n",
    "        targets, num_labels=num_labels\n",
    "        )\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "    mse = mse/i\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482020ad-ab8e-42a8-8e9f-54c5e3f5fdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
