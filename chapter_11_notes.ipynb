{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e9e985-964f-4670-83fc-f9ee782ae370",
   "metadata": {},
   "source": [
    "# Chapter 11 : Implementing a Multilayer Artificial Neural Network from Scratch\n",
    "\n",
    "## Modeling complex functions with artificial neural networks\n",
    "\n",
    "Note that a *neural network* originates in the 1940s with models such as ADALINE and the perceptron, already covered in this book.  Neural networks have become more populated after the ability to create *deep neural networks* which have multiple layers of neurons.\n",
    "\n",
    "## Single layer neural network recap\n",
    "\n",
    "For example, consider ADALINE.  Recall that in every epoch, the weight vector, $w$, and bias unit, $b$, are updated. Where\n",
    "$$\n",
    "    w := w + \\Delta w \\text{ and } b := b + \\Delta b\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta w_j = -\\eta \\frac{\\partial L}{\\partial w_j}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta b = -\\eta\\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "This is done through multiple passes over the training set where an activiation function (in ADALINE, the identity function) output is compared with the actual value.  Recall that this takes the opposite direction of the loss gradient ($\\nabla L(w)$) to find optimal weights of the model.  Generally $L$ is defined as the mean of square errors.  The model learning phase is accelerated by stochastic gradient descent.\n",
    "\n",
    "This will be used to implement and train a *multilayer perceptron* (MLP) model.\n",
    "\n",
    "## Introducing the multilayer neural network architecture\n",
    "\n",
    "A MLP model consists in an input layer, one or more hidden layers, and an output payer. If a network has more than one hidden layer, then it is said to be a *deep NN*. \n",
    "\n",
    "The $i$th activation unit in the $l$th later is denoted $a_i^{(l)}$.  Numerical units are usally not used for the layers; therfore, $x_i^{(in)}$ refers to the $i$th input feature value, $a_i^{(h)}$ refers to the $i$th unit in the hidden layer, and $a_i^{(out)}$ refers to the $i$th unit in the output layer. $b^{(h)}$, $b^{(out)}$ denote bias unit vectors storing $d$, the number of nodes, bias units for the hidden and output layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f9e0c-d5b0-4d13-b7ce-0e1b1918460e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
