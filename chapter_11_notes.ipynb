{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e9e985-964f-4670-83fc-f9ee782ae370",
   "metadata": {},
   "source": [
    "# Chapter 11 : Implementing a Multilayer Artificial Neural Network from Scratch\n",
    "\n",
    "## Modeling complex functions with artificial neural networks\n",
    "\n",
    "Note that a *neural network* originates in the 1940s with models such as ADALINE and the perceptron, already covered in this book.  Neural networks have become more populated after the ability to create *deep neural networks* which have multiple layers of neurons.\n",
    "\n",
    "### Single layer neural network recap\n",
    "\n",
    "For example, consider ADALINE.  Recall that in every epoch, the weight vector, $w$, and bias unit, $b$, are updated. Where\n",
    "$$\n",
    "    w := w + \\Delta w \\text{ and } b := b + \\Delta b\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta w_j = -\\eta \\frac{\\partial L}{\\partial w_j}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta b = -\\eta\\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "This is done through multiple passes over the training set where an activiation function (in ADALINE, the identity function) output is compared with the actual value.  Recall that this takes the opposite direction of the loss gradient ($\\nabla L(w)$) to find optimal weights of the model.  Generally $L$ is defined as the mean of square errors.  The model learning phase is accelerated by stochastic gradient descent.\n",
    "\n",
    "This will be used to implement and train a *multilayer perceptron* (MLP) model.\n",
    "\n",
    "### Introducing the multilayer neural network architecture\n",
    "\n",
    "A MLP model consists in an input layer, one or more hidden layers, and an output payer. If a network has more than one hidden layer, then it is said to be a *deep NN*. \n",
    "\n",
    "The $i$th activation unit in the $l$th later is denoted $a_i^{(l)}$.  Numerical units are usally not used for the layers; therfore, $x_i^{(in)}$ refers to the $i$th input feature value, $a_i^{(h)}$ refers to the $i$th unit in the hidden layer, and $a_i^{(out)}$ refers to the $i$th unit in the output layer. $b^{(h)}$, $b^{(out)}$ denote bias unit vectors storing $d$, the number of nodes, bias units for the hidden and output layers.\n",
    "\n",
    "### Activating a neural network via forward propagation\n",
    "\n",
    "The MLP procedure is summarized in three steps:\n",
    "1. Starting at $x_i^{(in)}$, patterns are forward propagated through the network\n",
    "2. Based on network output, calculate the loss we want to minimize using a loss function\n",
    "3. Back propogate the loss, finding the derivative of each weight and bias unity, updating the model\n",
    "These steps are repeated for each epoch. \n",
    "\n",
    "Moving step by step as follows.  First, calcualte the activation unit for the hidden layer as follows:\n",
    "$$\n",
    "    z_1^{(h)} = x_1^{(in)}w_{1,1}^{(h)} + \\dots + x_m^{(in)}w_{1,m}^{(h)}\n",
    "$$\n",
    "thus,\n",
    "$$\n",
    "    a_1^{(h)} = \\sigma(z_1^{(h)})\n",
    "$$\n",
    "Note that complex problems need a nonlinear activation function.  For example, the logistic regression's sigmoid activation function.\n",
    "\n",
    "The calculation can be generalized to $n$ examples as follows:\n",
    "$$\n",
    "    Z^{(h)}= X^{(in)}W^{{h)T} + b^{(h)}\n",
    "$$\n",
    "Thus:\n",
    "$$\n",
    "    A^{(h)} = \\sigma(Z^{(h)})\n",
    "$$\n",
    "And finally:\n",
    "$$\n",
    "    A^{(out)} = \\sigma(Z^{(out)})\n",
    "$$\n",
    "\n",
    "## Classifying handwritten digits\n",
    "\n",
    "Get data set from sci-kit learn.print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6f9e0c-d5b0-4d13-b7ce-0e1b1918460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml \n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048490a3-fd70-47e2-bdca-f1a1dbaf30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
