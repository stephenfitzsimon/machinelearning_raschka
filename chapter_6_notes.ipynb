{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b958fb2-027a-4663-9cac-bd3ac543a035",
   "metadata": {},
   "source": [
    "# Chapter 6: Learning best practices for model evaluation and hyperparameter tuning\n",
    "\n",
    "## Streamlining workflows with pipelines\n",
    "\n",
    "Piplines allow for fitting a model usign an arbitrary number of transformations steps\n",
    "\n",
    "### reading in the data:\n",
    "\n",
    "1. get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf894637-8241-48f5-b863-c0cf0ff4b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wdbc.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696ed97-5dbc-46cd-a6a8-6b5fc361464c",
   "metadata": {},
   "source": [
    "2. use label encoder to transform the data into numerics\n",
    "\n",
    "(M = malignant tumors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d3be76-e0ee-4542-b463-9470a155eede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'M'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "X = df.iloc[:, 2:].values\n",
    "y = df.iloc[:, 1].values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f342ae4-aa64-40d7-ae22-81fc16e743bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(['M', 'B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dedfb00-1a11-409f-94eb-71728f249782",
   "metadata": {},
   "source": [
    "3. split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4451736-e6b4-42b4-b4c6-3290a51f4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905fd46-fd1d-4c91-a43c-ba4129981883",
   "metadata": {},
   "source": [
    "### Now apply the transforms with pipeline\n",
    "\n",
    "Data will need to be standardized.  And, to reduce the 30 features, PCA will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90dadb6-91d0-455f-9be8-38723a23d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import make_pipeline #this will make the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229e780-5f1b-43bd-8762-edf5a56c52a1",
   "metadata": {},
   "source": [
    "Make a pipeline using the scaler, PCA dimensionality reductiona, and then apply the model. `make_pipeline` will take an arbitrary number of transformer objects which are followed by an estimator. The pipeline object then acts like a \"meta-estimator\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54e5e27-584b-4e92-8a36-bb7e1eee5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c3b05-191c-4c5d-b7e8-effd554c1f07",
   "metadata": {},
   "source": [
    "`pipe_lr` will now act like a model object for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44805fa6-4061-4cab-b1ee-0e59fd1b09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.930\n"
     ]
    }
   ],
   "source": [
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "test_acc = pipe_lr.score(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a030208e-c21f-4d8a-ac65-a98fc76cc58b",
   "metadata": {},
   "source": [
    "The `pipeline.fit()` will be used with training data and the `pipeline.predict()` is used with test data.  The training data passes through the `fit` and `transform` methods of the transformers/estimators, whereas the test data only passes through the `transform` methods.\n",
    "\n",
    "## Using k-fold cross validation to assess model performance\n",
    "\n",
    "### Holdout method\n",
    "\n",
    "*Holdout cross-validation* is when a test set is held out to test a model on unseen data. However, this can be extended to holding out a validation set on which to tune hyperparameters for a model, and to help with model selection. Then the final model and hyperparameters are selected and used to predict on the test data set.  However, models might be sensitive to how the train/validate/test are partitioned.\n",
    "\n",
    "### K-fold cross validation\n",
    "\n",
    "*K-fold cross validation* is a more robust way of cross validation. The training set is subset into $k$ folds without replacement, and $k - 1$ are training folds with the last fold as the test fold which is used for performance evaluation. This is repeated $k$ times for $k$ models.\n",
    "\n",
    "Each model is then hyperparameter tuned to a single fold of the $k-1$ train folds and tested on the test fold. Once hyperparameters are tuned, the model is trained on the entire train set and evaluated on the test set.  Then all of the $k$ models (having been hyperparameter tuned) can be evaluated against the test set.\n",
    "\n",
    "Estimated performance can be computed for the $k$ folds by:\n",
    "$$\n",
    "    E = \\sum^k_{i=0} \\frac{E_i}{k}\n",
    "$$\n",
    "Where $E_i$ is the evaluation metric for each of the models.\n",
    "\n",
    "K-fold validation uses all the datapoints, unlike the holdout method.  However, an efficient choice of $k$ should be chosen; a good standard is 10.\n",
    "\n",
    "(Note: each model gets a single fold to train on.  Only the final model is trained on the entire train fold.  Before the final model, each model is trained on $\\frac{1}{k}$ of the data).\n",
    "\n",
    "Smaller datasets can benefit from a larger $k$, so that more of the data is used in each iteration.  This increases runtime and variance. Larger datasets  benefit from a smaller $k$ because in decreases runtime.\n",
    "\n",
    "*Leave one out cross-validation* : set $k=n$, and leave a single record out to predict.  Every iteration, a different record is left out to predict.  This is useful for small datasets.\n",
    "\n",
    "*Stratified k-fold cross-validation* : target labels are stratified in the training and test sets. Use scikit-learn's `StratifiedKFold`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c731d69f-da53-4940-bce8-f1360af30eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 01, Class distr.: [256 152], Acc.: 0.913\n",
      "Fold: 02, Class distr.: [256 152], Acc.: 0.935\n",
      "Fold: 03, Class distr.: [256 152], Acc.: 0.957\n",
      "Fold: 04, Class distr.: [256 152], Acc.: 0.891\n",
      "Fold: 05, Class distr.: [257 152], Acc.: 0.978\n",
      "Fold: 06, Class distr.: [257 152], Acc.: 0.978\n",
      "Fold: 07, Class distr.: [257 152], Acc.: 0.978\n",
      "Fold: 08, Class distr.: [257 152], Acc.: 0.911\n",
      "Fold: 09, Class distr.: [257 152], Acc.: 0.933\n",
      "Fold: 10, Class distr.: [256 153], Acc.: 0.978\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train) \n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test]) \n",
    "    scores.append(score)\n",
    "    print(f\"Fold: {k+1:02d}, Class distr.: {np.bincount(y_train[train])}, Acc.: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675473d-d7b8-4714-8c8c-0b50d44a1db8",
   "metadata": {},
   "source": [
    "- `n_splits` = $k$\n",
    "- `.split()` is where the target variable is defined\n",
    "- Then the splits are looped over for training and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aab07a4-94a8-4c52-a60f-e022905077f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV accuracy: 0.945 +/- 0.031\n"
     ]
    }
   ],
   "source": [
    "mean_acc = np.mean(scores)\n",
    "std_acc = np.std(scores)\n",
    "print(f'\\nCV accuracy: {mean_acc:.3f} +/- {std_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f933cc4-1e06-4fde-8e19-ae013ef6425d",
   "metadata": {},
   "source": [
    "The average metrics are calculated. `cross_val_score` can also be used, where `cv` is the number of splits.  Note that `n_jobs` will distribute the computation across multiple CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33768952-9b12-4751-8a58-2712ab1f34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [0.91304348 0.93478261 0.95652174 0.89130435 0.97777778 0.97777778\n",
      " 0.97777778 0.91111111 0.93333333 0.97777778]\n",
      "CV accuracy: 0.945 +/- 0.031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1)\n",
    "print(f'CV accuracy scores: {scores}')\n",
    "print(f'CV accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85dfa3-64c9-469a-921c-ca5a839445a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
